{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection**\n",
    "\n",
    "#### All datasets: https://github.com/shuhashi0352/Japanese-Politeness-Classification/tree/main/Datasets\n",
    "\n",
    "### **Collecting Data from KeiCo Corpus**\n",
    "\n",
    "I mostly collected sentences from the KeiCO Corpus, which was originally developed for the project titled Construction and Validation of a Japanese Honorific Corpus Based on Systemic Functional Linguistics (https://aclanthology.org/2022.dclrl-1.3/). This corpus was selected because it is systematically organised by levels of politeness, with each sentence already annotated for its degree of honorific usage. However, since the labeling scheme used in the corpus differs slightly from the one adopted in this project, several modifications were made:\n",
    "\n",
    "* Although the original corpus includes approximately 10,000 sentences, the number of sentences used in this project was reduced to 4,000 to reduce annotation effort and improve manageability.\n",
    "\n",
    "* The original four-level scale in the KeiCO Corpus was mapped onto a three-class system for this project: sentences labeled as Level 1 (the highest honorific level) and Level 2 (secondary honorific level) were merged and assigned to the Polite category (Level 1 in this project’s terms).\n",
    "\n",
    "* Sentences from Level 3 were excluded, as they often include both polite and impolite expressions, making the classification boundary too ambiguous for consistent annotation.\n",
    "\n",
    "* Sentences from Level 4, which contain primarily impolite expressions, were retained and relabeled as Impolite (Level 3).\n",
    "\n",
    "* For the Neutral (Level 2) category, additional sentences were independently sampled from Japanese Wikipedia to ensure domain separation and to represent descriptive, non-stylised language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Collecting Data from Wikipedia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm randomly selects a Wikipedia page written in Japanese. After extracting the content line by line, each line is appended to a text file for storage. \n",
    "\n",
    "During this extraction process, specific types of lines are excluded using regular expressions to ensure the quality and relevance of the collected data.\n",
    "\n",
    "The line to begin with:\n",
    "\n",
    "* `0-9` \n",
    "\n",
    "* `=`\n",
    "\n",
    "* `ISBN` \n",
    "\n",
    "* `http`\n",
    "\n",
    "* `www`\n",
    "\n",
    "The chances are pretty low that they are sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Set language to Japanese\n",
    "wikipedia.set_lang(\"ja\") \n",
    "\n",
    "random_title = wikipedia.random()\n",
    "\n",
    "try:\n",
    "    page = wikipedia.page(random_title)\n",
    "\n",
    "    # Read the input text\n",
    "    input_text = page.content\n",
    "\n",
    "    # Split text into sentences based on \"。\" (full stop)\n",
    "    sentences = input_text.split(\"。\")\n",
    "\n",
    "    # Remove the first sentence (title) and filter out sentences starting with any numbers or \"=|ISBN|http|www\".\n",
    "    filtered_sentences = [\n",
    "        sentence.strip() for i, sentence in enumerate(sentences) if i > 0 and len(sentence) >= 10\n",
    "        # Remove sentences starting with numbers\n",
    "        and not re.match(r\"^\\d+\", sentence.strip()) \n",
    "        # IGNORECASE: Case-insensitive URL/ISBN check\n",
    "        and not re.match(r'^(=|ISBN|http|www)', sentence.strip(), re.IGNORECASE)]\n",
    "\n",
    "    # Join the remaining lines back into a single text\n",
    "    filtered_text = \"。\\n\".join(filtered_sentences)\n",
    "\n",
    "    # Ensure the last sentence ends with \"。\"\n",
    "    if not filtered_text.endswith(\"。\"):\n",
    "        filtered_text += \"。\"\n",
    "        print(filtered_text)\n",
    "\n",
    "    # Append to file \n",
    "    # with open(\"wiki_appended.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "    #     file.write(\"\\n\" + filtered_text) \n",
    "\n",
    "    # print(\"Filtered text appended to 'wiki_appended.txt'\")\n",
    "\n",
    "except wikipedia.exceptions.PageError:\n",
    "    print(\"Page not found.\")\n",
    "except wikipedia.exceptions.DisambiguationError as e:\n",
    "    print(f\"Disambiguation Error. Options: {e.options}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting up to 2,000 sentences from Wikipedia, the resulting text file is merged with an existing CSV file containing 4,000 sentences from Levels 1 and 2 of the KeiCO Corpus, forming a unified dataset for politeness classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "txt_file = \"wiki_appended.txt\"\n",
    "csv_file = \"keico_corpus(forLREC)-OldVersion_all.csv\"\n",
    "\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as txt, open(csv_file, \"a\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    for line in txt:\n",
    "        cleaned_line = line.strip()\n",
    "        if cleaned_line: \n",
    "            csv_writer.writerow([cleaned_line])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
