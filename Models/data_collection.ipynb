{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Collection**\n",
    "\n",
    "I mostly collected sentences from `KeiCo Corpus` which was previously used for the project called **Construction and Validation of a Japanese Honorific Corpus Based on Systemic Functional Linguistics** (https://aclanthology.org/2022.dclrl-1.3/). The reason for choosing this corpus was that this corpus is organized in order of politeness, and the level of politeness has been already labeled for every sentence. However, the labeling methodology is slightly different from the one on this project, so I modified the corpus in the following ways.\n",
    "\n",
    "* Although the corpus contains around 10000 sentences, the number of sentences collected are reduced to 4000 sentences in total in order to ease strain on annotation.\n",
    "\n",
    "* There were four levels assigned in the corpus. However, since this project divides the type of politeness into three levels (Polite = Level 1 / Neutral = Level 2 / Impolite = Level 3), the sentences with level 1 (The highest honorific level) and level 2 (Secondary honorific level) are merged to level 1 on the dataset used for the project. \n",
    "\n",
    "* The level 3 sentences were eliminated from the dataset for the project because level 3 contains both polite and impolite expressions, making the border for politeness ambiguous. \n",
    "\n",
    "* The sentences at level 4, which includes only impolite expressions, were used for the dataset at level 3.\n",
    "\n",
    "* For neutral sentences (level 2), the text on Wikipedia is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Collecting Data on Wikipedia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm below randomly choses a Wikipedia page written in Japanese.\n",
    "Once text is extracted line by line, all the lines are appended to txt file.\n",
    "\n",
    "In the process of extraction, the following types of lines are filtered out by regular expression.\n",
    "\n",
    "The line starting with:\n",
    "\n",
    "* `0-9` \n",
    "\n",
    "* `=`\n",
    "\n",
    "* `ISBN` \n",
    "\n",
    "* `http`\n",
    "\n",
    "* `www`\n",
    "\n",
    "They are likely not to be sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Set language to Japanese\n",
    "wikipedia.set_lang(\"ja\") \n",
    "\n",
    "random_title = wikipedia.random()\n",
    "\n",
    "try:\n",
    "    page = wikipedia.page(random_title)\n",
    "\n",
    "    # Read the input text\n",
    "    input_text = page.content\n",
    "\n",
    "    # Split text into sentences based on \"。\" (full stop)\n",
    "    sentences = input_text.split(\"。\")\n",
    "\n",
    "    # Remove the first sentence (title) and filter out sentences starting with any numbers or \"=|ISBN|http|www\".\n",
    "    filtered_sentences = [\n",
    "        sentence.strip() for i, sentence in enumerate(sentences) if i > 0 and len(sentence) >= 10\n",
    "        # Remove sentences starting with numbers\n",
    "        and not re.match(r\"^\\d+\", sentence.strip()) \n",
    "        # IGNORECASE: Case-insensitive URL/ISBN check\n",
    "        and not re.match(r'^(=|ISBN|http|www)', sentence.strip(), re.IGNORECASE)]\n",
    "\n",
    "    # Join the remaining lines back into a single text\n",
    "    filtered_text = \"。\\n\".join(filtered_sentences)\n",
    "\n",
    "    # Ensure the last sentence ends with \"。\"\n",
    "    if not filtered_text.endswith(\"。\"):\n",
    "        filtered_text += \"。\"\n",
    "        print(filtered_text)\n",
    "\n",
    "    # Append to file \n",
    "    # with open(\"wiki_appended.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "    #     file.write(\"\\n\" + filtered_text) \n",
    "\n",
    "    # print(\"Filtered text appended to 'wiki_appended.txt'\")\n",
    "\n",
    "except wikipedia.exceptions.PageError:\n",
    "    print(\"Page not found.\")\n",
    "except wikipedia.exceptions.DisambiguationError as e:\n",
    "    print(f\"Disambiguation Error. Options: {e.options}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting text up to 2000 sentences, the txt file is merged to the csv file, which has already contained 4000 sentences (level 1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "txt_file = \"wiki_appended.txt\"\n",
    "csv_file = \"keico_corpus(forLREC)-OldVersion_all.csv\"\n",
    "\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as txt, open(csv_file, \"a\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    for line in txt:\n",
    "        cleaned_line = line.strip()\n",
    "        if cleaned_line: \n",
    "            csv_writer.writerow([cleaned_line])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
